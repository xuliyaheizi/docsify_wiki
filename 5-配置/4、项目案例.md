# 案例

后端: 

1. nginx日志flume采集  hdfs

   ```shell
   #Agent配置
   n1.sources=r1
   n1.sinks=k1
   n1.channels=c1
   
   #配置Sources
   n1.sources.r1.type=exec
   n1.sources.r1.deserializer.outputCharset=UTF-8
   n1.sources.r1.batchSize=100
   n1.sources.r1.batchTimeout=2000
   
   #配置需要监控的日志输出目录
   n1.sources.r1.command=tail -F /usr/local/nginx/logs/access.log
   
   #配置Sink
   n1.sinks.k1.type=hdfs
   #按天分区
   n1.sinks.k1.hdfs.path=hdfs://master:8020/nginxlogs/log-access/%Y-%m-%d
   #文件名
   n1.sinks.k1.hdfs.filePrefix=%H-%M
   n1.sinks.k1.hdfs.fileSuffix=.log
   n1.sinks.k1.hdfs.round=true
   #10分钟一次
   n1.sinks.k1.hdfs.roundValue=10
   #指定每个HDFS块的最小配置数。如果没有指定，则来着类路径中的默认  Hadoop配置
   n1.sinks.k1.hdfs.minBlockReplicas=1
   n1.sinks.k1.hdfs.fileType=DataStream
   #重新定义时间单位
   n1.sinks.k1.hdfs.roundUnit=minute
   #是否使用本地时间戳
   n1.sinks.k1.hdfs.useLocalTimeStamp=true
   #积攒多少个Event才flush到hdfs一次
   n1.sinks.k1.hdfs.batchSize=100
   #设置文件类型，可支持压缩
   n1.sinks.k1.hdfs.fileType=DataStream
   #多久生成一个新的文件
   n1.sinks.k1.hdfs.rollInterval=600
   #设置每个文件的滚动大小
   n1.sinks.k1.hdfs.rollSize=134217700
   #文件的滚动与Event数量无关
   n1.sinks.k1.hdfs.rollCount=0
   
   #配置Channel
   n1.channels.c1.type=memory
   n1.channels.c1.capacity=1000
   n1.channels.c1.transactionCapacity=100
   
   #连接
   n1.sources.r1.channels=c1
   n1.sinks.k1.channel=c1
   ```

   

   ```shell
   #Agent配置
   n2.sources=r1
   n2.sinks=k1
   n2.channels=c1
   
   #配置Sources
   n2.sources.r1.type=exec
   n2.sources.r1.deserializer.outputCharset=UTF-8
   n2.sources.r1.batchSize=100
   n2.sources.r1.batchTimeout=2000
   
   #配置需要监控的日志输出目录
   n1.sources.r1.command=tail -F /usr/local/nginx/logs/error.log
   
   #配置Sink
   n2.sinks.k1.type=hdfs
   #按天分区
   n2.sinks.k1.hdfs.path=hdfs://master:8020/nginxlogs/log-error/%Y-%m-%d
   #文件名
   n2.sinks.k1.hdfs.filePrefix=%H-%M
   n2.sinks.k1.hdfs.fileSuffix=.log
   n2.sinks.k1.hdfs.round=true
   #10分钟一次
   n2.sinks.k1.hdfs.roundValue=10
   #指定每个HDFS块的最小配置数。如果没有指定，则来着类路径中的默认  Hadoop配置
   n2.sinks.k1.hdfs.minBlockReplicas=1
   n2.sinks.k1.hdfs.fileType=DataStream
   #重新定义时间单位
   n2.sinks.k1.hdfs.roundUnit=minute
   #是否使用本地时间戳
   n2.sinks.k1.hdfs.useLocalTimeStamp=true
   #积攒多少个Event才flush到hdfs一次
   n2.sinks.k1.hdfs.batchSize=100
   #设置文件类型，可支持压缩
   n2.sinks.k1.hdfs.fileType=DataStream
   #多久生成一个新的文件
   n2.sinks.k1.hdfs.rollInterval=600
   #设置每个文件的滚动大小
   n2.sinks.k1.hdfs.rollSize=134217700
   #文件的滚动与Event数量无关
   n2.sinks.k1.hdfs.rollCount=0
   
   #配置Channel
   n2.channels.c1.type=memory
   n2.channels.c1.capacity=1000
   n2.channels.c1.transactionCapacity=100
   
   #连接
   n2.sources.r1.channels=c1
   n2.sinks.k1.channel=c1
   ```

   程序日志flume采集  hdfs

   ```shell
   d1.sources=r1
   d1.sinks=k1
   d1.channels=c1
   
   #配置Sources
   d1.sources.r1.type=exec
   d1.sources.r1.command=tail -F /opt/cloudDisk/cloudDisk.log
   d1.sources.r1.shell=/bin/bash -c
   d1.sources.r1.batchSize=100
   d1.sources.r1.batchTimeout=2000
   
   #配置Sink
   d1.sinks.k1.type=hdfs
   d1.sinks.k1.hdfs.path=hdfs://master:8020/flume/cloudDiskLogs/%Y-%m-%d
   d1.sinks.k1.hdfs.filePrefix=node1log-%H-%M
   d1.sinks.k1.hdfs.fileSuffix=.log
   d1.sinks.k1.hdfs.round=true
   #10分钟一次
   d1.sinks.k1.hdfs.roundValue=10
   #指定每个HDFS块的最小配置数。如果没有指定，则来着类路径中的默认  Hadoop配置
   d1.sinks.k1.hdfs.minBlockReplicas=1
   d1.sinks.k1.hdfs.fileType=DataStream
   #重新定义时间单位
   d1.sinks.k1.hdfs.roundUnit=minute
   #是否使用本地时间戳
   d1.sinks.k1.hdfs.useLocalTimeStamp=true
   #积攒多少个Event才flush到hdfs一次
   d1.sinks.k1.hdfs.batchSize=100
   #设置文件类型，可支持压缩
   d1.sinks.k1.hdfs.fileType=DataStream
   #多久生成一个新的文件
   d1.sinks.k1.hdfs.rollInterval=60
   #设置每个文件的滚动大小
   d1.sinks.k1.hdfs.rollSize=134217700
   #文件的滚动与Event数量无关
   d1.sinks.k1.hdfs.rollCount=0
   
   #配置Channel
   d1.channels.c1.type=memory
   d1.channels.c1.capacity=1000
   d1.channels.c1.transactionCapacity=100
   
   #连接
   d1.sources.r1.channels=c1
   d1.sinks.k1.channel=c1
   ```

2. mysql8中的数据库日志
   a. 用户登录
   b. 用户与文件之间的关系
   sqoop 采集到 hdfs中

3. azkaban来调度以上的任务. 
   a. 自定义一个任务，自动生成一些测试数据
   b. 建议在采集任务运行前，先运行这个数据生成任务. 
   c. 书写 mr 任务计算一些指标. 
     网站的访问量, 客户端统计( 各种浏览器类型 , 移动端与pc端) , 
     数据文件的流行度. 
     ....
    存到一个文件 part000,

   ```
   hadoop jar /opt/mapreduce/cloud.jar com.zhulin.WebSite /nginxlogs/log-access/2022-07-01 /nginxlogs/log-access/out1
   hadoop jar /opt/mapreduce/cloud.jar com.zhulin.WebClient /nginxlogs/log-access/2022-07-01 /nginxlogs/log-access/out2
   hadoop jar /opt/mapreduce/cloud.jar com.zhulin.WebWeb /nginxlogs/log-access/2022-07-01 /nginxlogs/log-access/out3
   
   hadoop jar "jar包路径" com.zhulin.Webwork "监听日志路径" "输出访问量路径" "输出客户端类型路径" "输出Os类型路径"
   
   sqoop export --connect "数据库地址" --username "用户名" --password "密码" -export-dir "导入文件路径" --table cloud_disk_log --input-fields-terminated-by '\t' --columns="type,name,num,time"
   
   sqoop export --connect jdbc:mysql://master:3306/datastatic --username root --password aaaa -export-dir /nginxlogs/log-access/out2/part-r-00000 --table cloud_disk_log --input-fields-terminated-by '\t' --columns="type,name,num,time"
   
   sqoop export --connect jdbc:mysql://master:3306/datastatic --username root --password aaaa -export-dir /nginxlogs/log-access/out1/part-r-00000 --table cloud_disk_log --input-fields-terminated-by '\t' --columns="type,name,num,time"
   ```
   
   d. 写一个任务，读取上面的输出文件，将结果存到数据库中，供后端管理程序. 
   
4. 写一个spring boot后端程序, 将上面计算出来的指标以可视化的方案显示出来. 
   百度可视化  chart

5. ganglia监控, azkaban, namenode, resourcemanager 整合到一个界面. 
   htm: frameset


hadoop dfsadmin -safemode leave





#hadoop cluster
124.70.180.175 hadoop1 hadoop1001 
121.37.176.40 hadoop2 hadoop1002
121.36.215.98 hadoop3 hadoop1003
123.60.58.108 hadoop4 hadoop1004