# 云盘项目总结

1. node1:请安装 mysql, ngix, tomcat

2. 请配置 flume 采集 nginx, tomcat运行日志，入库到 hdfs中. 

3. 请配置好云盘项目的 日志, 发布你云盘项目,打成 jar包，运行起来, 
   用flume采集此项目的日志。 

4. 请将云盘项目动静分离. 静态资源 html, css, js放到 nginx中， 后台部分打成jar独立运行，生成日志，
   由flume采集日志发到 hdfs 保存. 负载均衡

## 一、Hadoop安装mysql、nginx、tomcat

### 1.1、mysql安装

下载安装包，将压缩包上传至 /opt/mysql，并解压

```shell
https://dev.mysql.com/downloads/file/?id=511379
tar -xf mysql-8.0.29-1.el7.x86_64.rpm-bundle.tar
```

添加用户组

```shell
groupadd mysql
添加用户组

useradd -g mysql mysql
添加用户

id mysql
查看用户信息。
```

卸载MariaDB

```shell
rpm -qa | grep mariadb查询是否有该软件
如果有，就卸载：rpm -e mariadb-libs-这里是你的版本，如果不能卸载（或报错）则采用强制卸载：rpm -e --nodeps mariadb-libs-这里是你的版本
```

安装Mysql

```shell
rpm -ivh *.rpm
安装过程要按照如下顺序（必须）进行：

     mysql-community-common-8.0.29-1.el7.x86_64.rpm
     mysql-community-client-plugins-8.0.29-1.el7.x86_64.rpm
     mysql-community-libs-8.0.29-1.el7.x86_64.rpm             --（依赖于common）
     mysql-community-client-8.0.29-1.el7.x86_64.rpm          --（依赖于libs）
     mysql-community-icu-data-files-8.0.29-1.el7.x86_64.rpm
     mysql-community-server-8.0.29-1.el7.x86_64.rpm         --（依赖于client、common）
    按照以上顺序进行一个个的安装，脚本如下：
    
    rpm -ivh mysql-community-server-5.7.16-1.el7.x86_64.rpm
    
期间缺少啥组件，安装啥
libaio.so.1()(64bit) is needed by mysql-community-embedded-compat-8.0.29-1.el7.x86_64
yum -y install libaio

libnuma.so.1()(64bit) is needed by mysql-community-embedded-compat-8.0.29-1.el7.x86_64
yum -y install numactlCopy to clipboardErrorCopied
mysql安装软件在/usr/share/mysql目录下

Mysql数据库创建在/var/lib/mysql目录下
```

我们进入到mysql这个目录中，更改一下权限：

```shell
cd /usr/share/mysql/
chown -R mysql:mysql .
```

启动mysql

```shell
service mysqld restart

//登录
mysql
报错：ERROR 1045 (28000): Access denied for user 'root'@'localhost' (using password: NO)  需添加权限

在/ect/my.cnf 的最后面加上一行：skip-grant-tables

service mysqld restartCopy to clipboardErrorCopied
```

初始化数据库

```shell
sudo mysqld --initialize --user=mysql
//查看临时生成的root用户密码
sudo cat /var/log/mysqld.log
//修改密码
mysql> set password = password("新密码");
```

### 1.2、安装nginx

安装相关依赖包

```shell
sudo yum -y install openssl openssl-devel pcre pcre-devel zlib zlib-devel gcc gcc-c++Copy to clipboardErrorCopied
```

将nginx文件上传到 /tmp/nginx

```shell
Ngnix下载地址：http://nginx.org/en/download.html
```

进入nginx目录下，执行以下命令

```shell
./configure   --prefix=/usr/local/nginx

make && make install
```

启动nginx

```shell
在/usr/local/nginx/sbin目录下执行  ./nginx

#查看启动情况
ps -ef |grep nginx
```

### 1.3、安装tomcat

下载tomcat8.5解压并上传至 /usr/local/tomcat

![image-20220625113353903](https://knowledgeimagebed.oss-cn-hangzhou.aliyuncs.com/img/image-20220625113353903.png)

进入tomcat下的conf目录，修改tomcat-users.xml

```shell
cd /usr/local/tomcat/conf
vim tomcat-users.xml
#添加以下内容
<role rolename="admin-gui"/>
<role rolename="manager-gui"/>
<user username="admin" password="admin" roles="admin-gui,manager-gui"/>
```

配置Tomcat用户管理页的远程访问 cd /usr/localtomcat/webapps/manager/META-INF/context.xml 将下面内容注释掉。

<img src="https://knowledgeimagebed.oss-cn-hangzhou.aliyuncs.com/img/image-20220625113918529.png" alt="image-20220625113918529" width="67%;" />

## 二、Flume采集nginx、tomcat运行日志

### 1.1、采集nginx日志

修改nginx配置

```shell
cd /usr/local/nginx/conf
vim nginx.conf
#将部分注释去掉
log_format  main  '$remote_addr - $remote_user [$time_local] "$request" '
                      '$status $body_bytes_sent "$http_referer" '
                      '"$http_user_agent" "$http_x_forwarded_for"';
#修改日志名
        #开启
        access_log logs/access.log main;
        #access_log  logs/host.access.log  main;

#重启nginx
nginx -s reload
```

编写Flume配置

```shell
#新建flume配置目录
mkdir nginx
cd nginx
pwd  # /usr/local/flume/jobs/nginx
vim nginxlog-hdfs.conf
```

```shell
#Agent配置
n1.sources=r1
n1.sinks=k1
n1.channels=c1

#配置Sources
n1.sources.r1.type=exec
n1.sources.r1.deserializer.outputCharset=UTF-8
n1.sources.r1.batchSize=100
n1.sources.r1.batchTimeout=2000

#配置需要监控的日志输出目录
n1.sources.r1.command=tail -F /usr/local/nginx/logs/access.log

#配置Sink
n1.sinks.k1.type=hdfs
#按天分区
n1.sinks.k1.hdfs.path=hdfs://master:8020/flume/nginxlogs/%Y-%m-%d
#文件名
n1.sinks.k1.hdfs.filePrefix=log-%H-%M
n1.sinks.k1.hdfs.fileSuffix=.log
n1.sinks.k1.hdfs.round=true
n1.sinks.k1.hdfs.roundValue=1
#指定每个HDFS块的最小配置数。如果没有指定，则来着类路径中的默认  Hadoop配置
n1.sinks.k1.hdfs.minBlockReplicas=1
n1.sinks.k1.hdfs.fileType=DataStream
#重新定义时间单位
n1.sinks.k1.hdfs.roundUnit=minute
#是否使用本地时间戳
n1.sinks.k1.hdfs.useLocalTimeStamp=true
#积攒多少个Event才flush到hdfs一次
n1.sinks.k1.hdfs.batchSize=100
#设置文件类型，可支持压缩
n1.sinks.k1.hdfs.fileType=DataStream
#表示hdfs多长时间切分一个文件
n1.sinks.k1.hdfs.rollInterval=60
#设置每个文件的滚动大小
n1.sinks.k1.hdfs.rollSize=134217700
#表示每隔10条数据切出来一个文件，如果设置为0表示不按数据条数切文件
n1.sinks.k1.hdfs.rollCount=0

#配置Channel
n1.channels.c1.type=memory
n1.channels.c1.capacity=1000
n1.channels.c1.transactionCapacity=100

#连接
n1.sources.r1.channels=c1
n1.sinks.k1.channel=c1
```

### 2.2、采集tomcat运行日志

```shell
t1.sources=r1
t1.sinks=k1
t1.channels=c1

#配置Sources
t1.sources.r1.type=exec
t1.sources.r1.command=tail -F /usr/local/tomcat/logs/localhost_access_log.txt
t1.sources.r1.shell=/bin/bash -c
t1.sources.r1.batchSize=100
t1.sources.r1.batchTimeout=2000

#配置Sink
t1.sinks.k1.type=hdfs
t1.sinks.k1.hdfs.path=hdfs://master:8020/flume/tomcat/%Y-%m-%d
t1.sinks.k1.hdfs.filePrefix=log-%H-%M
t1.sinks.k1.hdfs.round=true
t1.sinks.k1.hdfs.roundValue=1
#指定每个HDFS块的最小配置数。如果没有指定，则来着类路径中的默认  Hadoop配置
t1.sinks.k1.hdfs.minBlockReplicas=1
t1.sinks.k1.hdfs.fileType=DataStream
#重新定义时间单位
t1.sinks.k1.hdfs.roundUnit=minute
#是否使用本地时间戳
t1.sinks.k1.hdfs.useLocalTimeStamp=true
#积攒多少个Event才flush到hdfs一次
t1.sinks.k1.hdfs.batchSize=100
#设置文件类型，可支持压缩
t1.sinks.k1.hdfs.fileType=DataStream
#多久生成一个新的文件
t1.sinks.k1.hdfs.rollInterval=60
#设置每个文件的滚动大小
t1.sinks.k1.hdfs.rollSize=134217700
#文件的滚动与Event数量无关
t1.sinks.k1.hdfs.rollCount=0

#配置Channel
t1.channels.c1.type=memory
t1.channels.c1.capacity=1000
t1.channels.c1.transactionCapacity=100

#连接
t1.sources.r1.channels=c1
t1.sinks.k1.channel=c1
```

## 三、运行云盘项目

云盘项目通过nginx动静分离，静态资源部署在nginx上，动态资源打成jar部署在node1、node2、node3节点上负载均衡。

### 3.1、nginx动静分离

<img src="https://knowledgeimagebed.oss-cn-hangzhou.aliyuncs.com/img/image-20220627144511732.png" alt="image-20220627144511732" width="67%;" />

```shell
#Nginx配置
location /cloudDisk{
	root /home;
	index login.html login.htm;
}
```

### 3.2、负载均衡

springboot项目动态资源打jar包配置

```xml
<build>
    <plugins>
        <!--这个是springboot的默认编译插件，他默认会把所有的文件打包成一个jar-->
        <plugin>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-maven-plugin</artifactId>
            <version>2.6.3</version>
            <executions>
                <execution>
                    <goals>
                        <goal>repackage</goal>
                    </goals>
                </execution>
            </executions>
            <configuration>
                <mainClass>com.zhulin.CloudDiskMain</mainClass>
                <fork>true</fork>
                <addResources>true</addResources>
                <outputDirectory>${project.build.directory}/jar</outputDirectory>
            </configuration>
        </plugin>
        <!-- 打JAR包 -->
        <plugin>
            <groupId>org.apache.maven.plugins</groupId>
            <artifactId>maven-jar-plugin</artifactId>
            <version>3.2.2</version>
            <configuration>
                <!-- 不打包资源文件（配置文件和依赖包分开） -->
                <excludes>
                    <exclude>static/**</exclude>
                </excludes>
                <archive>
                    <manifest>
                        <addClasspath>true</addClasspath>
                        <!-- MANIFEST.MF 中 Class-Path 加入前缀 -->
                        <classpathPrefix>lib/</classpathPrefix>
                        <!-- jar包不包含唯一版本标识 -->
                        <useUniqueVersions>false</useUniqueVersions>
                        <!--指定入口类 -->
                        <mainClass>com.zhulin.CloudDiskMain</mainClass>
                    </manifest>
                    <manifestEntries>
                        <!--MANIFEST.MF 中 Class-Path 加入资源文件目录 -->
                        <Class-Path>./config/</Class-Path>
                    </manifestEntries>
                </archive>
                <outputDirectory>${project.build.directory}</outputDirectory>
            </configuration>
        </plugin>
    </plugins>
    <finalName>cloudDisk</finalName>
</build>
```

```shell
#Nginx配置
upstream  cloud.com {  #服务器集群名字   
	server  master:12888    weight=1; #服务器配置   weight是权重的意思，权重越大，分配的概率越大。  
	server  node1:12888     weight=2;
	server  node2:12888     weight=2;
	server  node3:12888     weight=2;
}

#session共享配置，跨域
location /cloud{
	proxy_pass http://cloud.com/cloudDisk;
	proxy_redirect default;
	proxy_cookie_path /cloudDisk /cloud;
}
```

### 3.3、采集cloudDisk运行日志

```shell
#cloudDisk.jar运行命令
nohup java -jar /opt/cloudDsik/cloudDisk.jar > /opt/cloudDisk/cloudDisk.log &
日志监控 /opt/cloudDisk/cloudDisk.log文件
```

```shell
d2.sources=r1
d2.sinks=k1
d2.channels=c1

#配置Sources
d2.sources.r1.type=exec
d2.sources.r1.command=tail -F /opt/cloudDisk/cloudDisk.log
d2.sources.r1.shell=/bin/bash -c
d2.sources.r1.batchSize=100
d2.sources.r1.batchTimeout=2000
#配置Sink
d2.sinks.k1.type=hdfs
d2.sinks.k1.hdfs.path=hdfs://master:8020/flume/cloudDiskLogs/%Y-%m-%d
d2.sinks.k1.hdfs.filePrefix=node2log-%H-%M
d2.sinks.k1.hdfs.fileSuffix=.log
d2.sinks.k1.hdfs.round=true
d2.sinks.k1.hdfs.roundValue=1
#指定每个HDFS块的最小配置数。如果没有指定，则来着类路径中的默认  Hadoop配置
d2.sinks.k1.hdfs.minBlockReplicas=1
d2.sinks.k1.hdfs.fileType=DataStream
#重新定义时间单位
d2.sinks.k1.hdfs.roundUnit=minute
#是否使用本地时间戳
d2.sinks.k1.hdfs.useLocalTimeStamp=true
#积攒多少个Event才flush到hdfs一次
d2.sinks.k1.hdfs.batchSize=100
#设置文件类型，可支持压缩
d2.sinks.k1.hdfs.fileType=DataStream
#多久生成一个新的文件
d2.sinks.k1.hdfs.rollInterval=600
#设置每个文件的滚动大小
d2.sinks.k1.hdfs.rollSize=134217728
#文件的滚动与Event数量无关
d2.sinks.k1.hdfs.rollCount=0

#配置Channel
d2.channels.c1.type=memory
d2.channels.c1.capacity=1000
d2.channels.c1.transactionCapacity=100

#连接
d2.sources.r1.channels=c1
d2.sinks.k1.channel=c1
```

```shell
#flume采集命令
nohup flume-ng agent --conf conf/ --conf-file /usr/local/flume/jobs/cloudDiskLog/cloudDiskLogs-hdfs.conf --name d2 &
```

### 3.4、Session共享

```shell
#Nginx配置 
location /cloud{
	proxy_pass http://cloud.com/cloudDisk;
	proxy_redirect default;
	proxy_cookie_path /cloudDisk /cloud;
}

#/cloud 请求代理至 http://cloud.com/cloudDisk  proxy_cookie_path设置统一至/cloud
```

**SpringBoot项目添加redis共享session**

```xml
<dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-data-redis</artifactId>
</dependency>
<dependency>
    <groupId>org.springframework.session</groupId>
    <artifactId>spring-session-data-redis</artifactId>
</dependency>
```

```java
@SpringBootApplication
@ServletComponentScan
@EnableRedisHttpSession(maxInactiveIntervalInSeconds = 300)  //设置session过期时间
public class CloudDiskMain {
    public static void main(String[] args) {
        SpringApplication.run(CloudDiskMain.class, args);
    }
}
```

-Dflume.monitoring.type=ganglia -Dflume.monitoring.hosts=172.168.0.10:8649

## 四、日志监控

### 4.1、ganglia安装（编译方式安装）

#### 服务端监控（安装gweb）

**安装gemtad**

```shell
yum -y install apr-devel apr-util check-devel cairo-devel pango-devel libxml2-devel rpm-build glib2-devel dbus-devel freetype-devel fontconfig-devel gcc gcc-c++ expat-devel python-devel libXrender-devel

yum install -y libart_lgpl-devel pcre-devel libtool
yum install  -y rrdtool rrdtool-devel
cd /usr/local
mkdir tools
cd tools
wget http://www.mirrorservice.org/sites/download.savannah.gnu.org/releases/confuse/confuse-2.7.tar.gz
tar zxvf confuse-2.7.tar.gz
cd confuse-2.7
./configure  --prefix=/usr/local/ganglia-tools/confuse CFLAGS=-fPIC --disable-nls --libdir=/usr/local/ganglia-tools/confuse/lib64
make && make install
cd /usr/local/tools/
wget https://sourceforge.net/projects/ganglia/files/ganglia%20monitoring%20core/3.7.2/ganglia-3.7.2.tar.gz
tar zxf ganglia-3.7.2.tar.gz
cd ganglia-3.7.2
./configure --prefix=/usr/local/ganglia --enable-gexec --enable-status --with-gmetad --with-libconfuse=/usr/local/ganglia-tools/confuse 
make && make install
cp gmetad/gmetad.init /etc/init.d/gmetad
ln -s /usr/local/ganglia/sbin/gmetad /usr/sbin/gmetad
```

**安装gweb**

```shell
yum install httpd httpd-devel php -y
yum -y install rsync
cd /usr/local/toosl
wget https://sourceforge.net/projects/ganglia/files/ganglia-web/3.7.2/ganglia-web-3.7.2.tar.gz
tar zxvf /tools/ganglia-web-3.7.2.tar.gz -C /var/www/html/
cd /var/www/html/
mv ganglia-web-3.7.2 ganglia
cd /var/www/html/ganglia/
useradd -M -s /sbin/nologin www-data
make install
chown apache:apache -R /var/lib/ganglia-web/
```

**修改配置**

```shell
#修改启动脚本
vi /etc/init.d/gmetad

GMETAD=/usr/sbin/gmetad  #这句话可以自行更改gmetad的命令，当然也能向我们前面做了软连接
start() {
    [ -f /usr/local/ganglia/etc/gmetad.conf  ] || exit 6  #这里将配置文件改成现在的位置，不然启动没反应
    
#创建rrds目录
mkdir /var/lib/ganglia/rrds -p
chown -R nobody:nobody  /var/lib/ganglia/rrds

#修改gmetad配置文件
vi /usr/local/ganglia/etc/gmetad.conf
data_source "master" 172.168.0.10:8649   #这也是我们以后经常修改的地方，""里面是组名称  后面是去哪个IP的那个端口去采集gmond数据
```

#### 客户端监控

```shell
yum -y install apr-devel apr-util check-devel cairo-devel pango-devel libxml2-devel rpm-build glib2-devel dbus-devel freetype-devel fontconfig-devel gcc gcc-c++ expat-devel python-devel libXrender-devel

yum install -y libart_lgpl-devel pcre-devel libtool
mkdir /usr/local/tools
cd /usr/local/tools
wget http://www.mirrorservice.org/sites/download.savannah.gnu.org/releases/confuse/confuse-2.7.tar.gz
tar zxvf confuse-2.7.tar.gz
cd confuse-2.7
./configure  --prefix=/usr/local/ganglia-tools/confuse CFLAGS=-fPIC --disable-nls --libdir=/usr/local/ganglia-tools/confuse/lib64
make && make install
cd /usr/local/tools
wget https://sourceforge.net/projects/ganglia/files/ganglia%20monitoring%20core/3.7.2/ganglia-3.7.2.tar.gz
tar zxvf ganglia-3.7.2.tar.gz
cd ganglia-3.7.2
./configure --prefix=/usr/local/ganglia --enable-gexec --enable-status  --with-libconfuse=/usr/local/ganglia-tools/confuse
make && make install
/usr/local/ganglia/sbin/gmond -t >/usr/local/ganglia/etc/gmond.conf
cp /tools/ganglia-3.7.2/gmond/gmond.init /etc/init.d/gmond
mkdir -p /usr/local/ganglia/var/run
/etc/init.d/gmond restart
```

#### 服务启动

```shell
#服务启动
systemctl restart httpd
systemctl start gmetad 
systemctl start gmond

#客户端启动
systemctl start gmond
```

